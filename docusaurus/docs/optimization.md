# Build Optimization & Efficiency

How Slurm Factory achieves extreme build efficiency through intelligent caching, layered compilation, and public buildcache distribution.

## Architecture Overview

Slurm Factory employs a **three-tier caching strategy** that dramatically reduces build times and ensures consistency across any Slurm version Ã— GCC compiler combination.

```mermaid
graph TD
    A[User Request] --> B{Compiler in Buildcache?}
    B -->|Yes| C[Download GCC from S3/CloudFront]
    B -->|No| D[Build GCC from Source<br/>~45 min]
    D --> E[Publish to S3 Buildcache]
    E --> C
    
    C --> F{Dependencies in Buildcache?}
    F -->|Yes| G[Download Slurm Deps from S3]
    F -->|No| H[Build Dependencies<br/>~30 min]
    H --> I[Publish Deps to S3]
    I --> G
    
    G --> J{Final Slurm in Buildcache?}
    J -->|Yes| K[Download Complete Slurm<br/>~2 min]
    J -->|No| L[Build Slurm Binary<br/>~5 min]
    L --> M[Publish to S3]
    M --> K
    
    K --> N[Extract & Deploy<br/>~30 sec]
    
    style C fill:#90EE90
    style G fill:#90EE90
    style K fill:#90EE90
    style N fill:#FFD700
```

## CI/CD Lifecycle: Three-Stage Pipeline

Our GitHub Actions workflows create a **dependency pyramid** that maximizes cache reuse:

```mermaid
graph LR
    subgraph "Stage 1: Compiler Buildcache"
        A1[GCC 15.2.0] --> BC1[(S3 Buildcache)]
        A2[GCC 14.2.0] --> BC1
        A3[GCC 13.4.0] --> BC1
        A4[GCC 12.5.0] --> BC1
        A5[GCC 11.5.0] --> BC1
        A6[GCC 10.5.0] --> BC1
        A7[GCC 9.5.0] --> BC1
        A8[GCC 8.5.0] --> BC1
        A9[GCC 7.5.0] --> BC1
    end
    
    subgraph "Stage 2: Dependencies Buildcache"
        BC1 --> B1[Slurm 25.11 Deps Ã— 8 Compilers]
        BC1 --> B2[Slurm 24.11 Deps Ã— 8 Compilers]
        BC1 --> B3[Slurm 23.11 Deps Ã— 8 Compilers]
        BC1 --> B4[Slurm 23.02 Deps Ã— 8 Compilers]
        B1 --> BC2[(S3 Buildcache)]
        B2 --> BC2
        B3 --> BC2
        B4 --> BC2
    end
    
    subgraph "Stage 3: Complete Slurm Packages"
        BC2 --> C1[Slurm 25.11 Ã— 8 Compilers]
        BC2 --> C2[Slurm 24.11 Ã— 8 Compilers]
        BC2 --> C3[Slurm 23.11 Ã— 8 Compilers]
        BC2 --> C4[Slurm 23.02 Ã— 8 Compilers]
        C1 --> BC3[(S3 Buildcache<br/>+ CDN)]
        C2 --> BC3
        C3 --> BC3
        C4 --> BC3
    end
    
    BC3 --> D[End Users:<br/>Instant Download]
    
    style BC1 fill:#FFE4B5
    style BC2 fill:#FFE4B5
    style BC3 fill:#90EE90
    style D fill:#FFD700
```

### Stage 1: Compiler Bootstrap (Build Once, Use Forever)

**Workflow:** `build-and-publish-compiler-buildcache.yml`

- **Builds:** 9 GCC compiler versions (7.5.0 â†’ 15.2.0)
- **Runtime:** ~45 minutes per compiler (parallel execution)
- **Frequency:** Once per compiler version (rarely updated)
- **Output:** Relocatable GCC compilers with gcc-runtime libraries
- **Storage:** Published to S3 at `s3://slurm-factory-spack-buildcache-4b670/build_cache/`

```bash
# Each compiler is self-contained and relocatable
gcc-7.5.0-compiler.tar.gz   # RHEL 7 compatible (glibc 2.17)
gcc-13.4.0-compiler.tar.gz  # Ubuntu 24.04 (glibc 2.39) - default
gcc-15.2.0-compiler.tar.gz  # Latest (glibc 2.40)
```

**Key Optimization:** Compilers are built once and reused across **all** Slurm versions and dependency combinations.

### Stage 2: Dependency Buildcache (Compiler-Specific, Slurm-Agnostic)

**Workflow:** `build-and-publish-slurm-deps-all-compilers.yml`

- **Builds:** Dependencies for each Slurm version Ã— compiler combination
- **Matrix:** 4 Slurm versions Ã— 8 compilers = 32 dependency sets
- **Runtime:** ~30 minutes per combination (downloads compiler from Stage 1)
- **Frequency:** When Slurm versions change or dependencies update
- **Dependencies:** OpenMPI, MySQL, hwloc, JSON-C, YAML, JWT, Lua, etc.

```bash
# Dependencies are compiler-specific but Slurm-agnostic
slurm-25.11-deps-gcc-13.4.0/   # All deps for Slurm 25.11 with GCC 13.4.0
slurm-24.11-deps-gcc-11.5.0/   # All deps for Slurm 24.11 with GCC 11.5.0
```

**Key Optimization:** Dependencies are built once per compiler and shared across multiple Slurm patch releases.

### Stage 3: Final Slurm Packages (Binary Distribution)

**Workflow:** `build-and-publish-all-packages.yml`

- **Builds:** Complete Slurm installations (binary + dependencies + modules)
- **Runtime:** ~5 minutes (downloads from Stages 1 & 2, builds only Slurm binary)
- **Frequency:** For each new Slurm release or configuration change
- **Output:** Production-ready tarballs with Lmod modules

```bash
# Final deployable packages
slurm-25.11-gcc-13.4.0-x86_64.tar.gz   # Complete installation
slurm-24.11-gcc-11.5.0-x86_64.tar.gz   # Different version/compiler
```

**Key Optimization:** Only Slurm itself is compiled; everything else is downloaded from cache.

## Buildcache Efficiency Metrics

### Without Buildcache (Traditional Spack)
```
First Build:     90 minutes  (compile GCC + deps + Slurm)
Second Build:    90 minutes  (no sharing between versions)
Third Build:     90 minutes  (no sharing between compilers)
Total Time:      4.5 hours for 3 combinations
```

### With Slurm Factory Buildcache
```
Stage 1 (one-time):    45 min  (build 1 compiler)
Stage 2 (per Slurm):   30 min  (build deps, reuse compiler)
Stage 3 (final):        5 min  (build Slurm, reuse everything)
Subsequent builds:      2 min  (download pre-built from S3/CDN)

Total for first combo:  80 min
Total for 2nd combo:     2 min  (if compiler/deps exist)
Total for 3rd combo:     2 min  (if compiler/deps exist)
Total Time:             84 minutes for 3 combinations (94% reduction)
```

### Cache Hit Rates in Production

| Scenario | Cache Hits | Build Time | Speedup |
|----------|------------|------------|---------|
| **First-time user, popular combo** (Slurm 25.11 + GCC 13.4.0) | Compiler + Deps + Slurm | ~2 min | **45x faster** |
| **New Slurm version, existing compiler** (Slurm 25.11 + GCC 13.4.0) | Compiler + Deps | ~5 min | **18x faster** |
| **New compiler, existing Slurm** (Slurm 25.11 + GCC 16.0.0) | None | ~80 min | **1x (builds cache)** |
| **Rebuild same config** | Everything | ~2 min | **45x faster** |

## Consistency Across Configurations

### Supported Matrix: 4 Slurm Ã— 9 GCC = 36 Combinations

| Slurm Version | GCC Versions | Total Configs |
|---------------|-------------|---------------|
| **25.11** (latest) | 7.5.0, 8.5.0, 9.5.0, 10.5.0, 11.5.0, 12.5.0, 13.4.0, 14.2.0, 15.2.0 | 9 |
| **24.11** (stable) | 7.5.0, 8.5.0, 9.5.0, 10.5.0, 11.5.0, 12.5.0, 13.4.0, 14.2.0, 15.2.0 | 9 |
| **23.11** (LTS) | 7.5.0, 8.5.0, 9.5.0, 10.5.0, 11.5.0, 12.5.0, 13.4.0, 14.2.0, 15.2.0 | 9 |
| **23.02** (legacy) | 7.5.0, 8.5.0, 9.5.0, 10.5.0, 11.5.0, 12.5.0, 13.4.0, 14.2.0, 15.2.0 | 9 |

**Every combination produces:**
- âœ… Identical module structure (Lmod-based)
- âœ… Consistent RPATH configuration (no LD_LIBRARY_PATH needed)
- âœ… Relocatable binaries (extract anywhere)
- âœ… Same dependency versions (per Slurm release)
- âœ… Predictable file layout (`/opt/slurm/view/`)

### Configuration Consistency Examples

```bash
# Load Slurm 25.11 with GCC 13.4.0
module load slurm/25.11-gcc-13.4.0
which scontrol  # /opt/slurm/view/bin/scontrol

# Switch to GCC 11.5.0 build (same Slurm version)
module swap slurm/25.11-gcc-11.5.0
which scontrol  # /opt/slurm/view/bin/scontrol (same path, different binary)

# Switch to Slurm 24.11 (same compiler)
module swap slurm/24.11-gcc-13.4.0
which scontrol  # /opt/slurm/view/bin/scontrol (same path, different version)
```

**All combinations behave identically:**
- Same configuration file paths (`/etc/slurm/slurm.conf`)
- Same environment variables (`SLURM_CONF`, `SLURM_ROOT`)
- Same service management (systemd units)
- Same module metadata (version, compiler, architecture)

## Public Buildcache Distribution

### Infrastructure

- **S3 Bucket:** `s3://slurm-factory-spack-buildcache-4b670/`
- **CloudFront CDN:** `https://slurm-factory-spack-binary-cache.vantagecompute.ai`
- **Public Access:** Unauthenticated HTTP/HTTPS (no AWS credentials needed)
- **Global Edge:** CloudFront serves from nearest location
- **Bandwidth:** Free egress for open-source usage

### Spack Integration

Slurm Factory automatically configures Spack to use the public buildcache:

```yaml
# Auto-generated in spack.yaml
mirrors:
  slurm-factory:
    url: https://slurm-factory-spack-binary-cache.vantagecompute.ai/build_cache
    signed: false

config:
  install_tree:
    padded_length: 128  # Relocatable binaries
```

**No manual mirror setup required** - every build checks the buildcache first.

### Download Flow

```mermaid
sequenceDiagram
    participant User
    participant Spack
    participant CloudFront
    participant S3
    
    User->>Spack: slurm-factory build
    Spack->>CloudFront: GET /build_cache/linux-ubuntu24.04-x86_64/gcc-13.4.0/index.json
    CloudFront->>S3: Cache miss?
    S3-->>CloudFront: index.json
    CloudFront-->>Spack: index.json (cached)
    
    Spack->>CloudFront: GET /build_cache/.../slurm-25-11-0-1.spack
    CloudFront-->>Spack: Binary package (cached at edge)
    
    Spack->>User: Extract & install (2 minutes)
    
    Note over CloudFront: Subsequent requests<br/>served from edge cache
```

## Build Artifact Optimization

### Package Size Comparison

| Package Type | Size | Compression | Distribution |
|--------------|------|-------------|--------------|
| **Source Slurm** | ~12 MB | tar.bz2 | schedmd.com |
| **Compiled Slurm (static)** | ~500 MB | Uncompressed | Custom builds |
| **Slurm Factory (shared libs)** | ~180 MB | tar.gz | S3 + CDN |
| **Full installation (with deps)** | ~450 MB | tar.gz | S3 + CDN |

### Storage Efficiency

```bash
# Traditional approach: 36 full builds
36 configs Ã— 450 MB = 16.2 GB total storage

# Slurm Factory approach: layered caching
9 compilers Ã— 180 MB  = 1.6 GB  (Stage 1)
32 dep sets Ã— 200 MB  = 6.4 GB  (Stage 2)
36 Slurm pkgs Ã— 50 MB = 1.8 GB  (Stage 3 - only Slurm binary)
Total storage:          9.8 GB  (40% reduction)
```

**Deduplication:** Shared dependencies (OpenMPI, MySQL) are stored once per compiler, not per Slurm version.

## Local Build Optimization

Even when building locally (not using public buildcache), Slurm Factory caches intelligently:

### Local Cache Structure

```bash
~/.slurm-factory/
â”œâ”€â”€ spack-buildcache/      # Downloaded binaries from S3
â”‚   â””â”€â”€ linux-ubuntu24.04-x86_64/
â”œâ”€â”€ spack-sourcecache/     # Source tarballs (never re-downloaded)
â”‚   â”œâ”€â”€ slurm-25.11.tar.bz2
â”‚   â””â”€â”€ gcc-13.4.0.tar.gz
â”œâ”€â”€ compilers/             # Built compilers (reused across Slurm versions)
â”‚   â”œâ”€â”€ gcc-13.4.0/
â”‚   â””â”€â”€ gcc-11.5.0/
â””â”€â”€ builds/                # Final outputs
    â””â”€â”€ slurm-25.11-gcc-13.4.0/
```

### Build Performance (Local)

```bash
# First build: Downloads from S3 buildcache
$ time slurm-factory build --slurm-version 25.11 --compiler-version 13.4.0
real    2m15s  # Download + extract

# Different Slurm version, same compiler: Reuses compiler, downloads Slurm deps
$ time slurm-factory build --slurm-version 24.11 --compiler-version 13.4.0
real    1m45s  # Compiler cached locally

# Same Slurm, different compiler: Downloads new compiler + deps
$ time slurm-factory build --slurm-version 25.11 --compiler-version 11.5.0
real    2m30s  # New compiler from buildcache

# Rebuild same config: Everything cached
$ time slurm-factory build --slurm-version 25.11 --compiler-version 13.4.0
real    0m45s  # All local caches hit
```

## Production Deployment Efficiency

### Parallel Deployment Across Cluster

```bash
# Deploy to 100 compute nodes in parallel (using Ansible/parallel-ssh)
time ansible compute -m copy -a "src=slurm-25.11-gcc-13.4.0.tar.gz dest=/tmp/"
# ~2 minutes (network I/O)

time ansible compute -m shell -a "tar -xzf /tmp/slurm-25.11-gcc-13.4.0.tar.gz -C /opt/"
# ~30 seconds (parallel extraction)

time ansible compute -m shell -a "systemctl restart slurmd"
# ~10 seconds (parallel restart)

# Total cluster update time: ~3 minutes for 100 nodes
```

### Incremental Updates

When upgrading Slurm versions, only changed files are updated:

```bash
# Upgrade from 25.11 to 25.11 (same compiler)
rsync -av --delete slurm-25.11-gcc-13.4.0/ /opt/slurm/
# Only Slurm binaries changed (~50 MB)
# Dependencies unchanged (~400 MB reused)
```

## Why This Approach is Superior

### Compared to Package Managers (apt/yum)

| Feature | apt/yum | Slurm Factory |
|---------|---------|---------------|
| **Slurm Versions** | 1-2 (distro-provided) | 4 (upstream releases) |
| **GCC Versions** | 1 (system default) | 9 (7.5.0 â†’ 15.2.0) |
| **Combinations** | 1-2 | 36 |
| **Update Lag** | 6-12 months | 0 (same-day releases) |
| **Relocatable** | No (hardcoded paths) | Yes (extract anywhere) |
| **Consistency** | Varies by distro | Identical everywhere |

### Compared to EasyBuild/Spack Manual

| Feature | Manual Spack | Slurm Factory |
|---------|--------------|---------------|
| **First Build Time** | 90 min | 2 min (buildcache) |
| **Configuration** | Manual spack.yaml | Auto-generated |
| **Dependency Conflicts** | Common (manual resolution) | Never (curated specs) |
| **Module System** | Manual setup | Auto-configured Lmod |
| **Reproducibility** | Environment-dependent | Guaranteed (Docker + buildcache) |
| **Public Distribution** | None | Free S3 + CDN |

### Compared to Pre-built Containers

| Feature | Docker/Singularity | Slurm Factory |
|---------|-------------------|---------------|
| **Size** | 2-4 GB (full OS) | 450 MB (just software) |
| **Flexibility** | Monolithic | Modular (swap versions) |
| **System Integration** | Isolated | Native (systemd, cgroups) |
| **Multiple Versions** | Separate containers | Parallel modules |
| **Update Process** | Rebuild entire container | Swap tarball |

## Performance Tuning Tips

### For CI/CD Pipelines

```yaml
# GitHub Actions: Use self-hosted runners with local caches
runs-on: self-hosted  # Persistent disk caches
timeout-minutes: 480  # Allow long builds

# Fail-fast: false = build all combinations even if one fails
strategy:
  fail-fast: false
  matrix:
    slurm_version: ["25.11", "24.11", "23.11", "23.02"]
    compiler_version: ["13.4.0", "11.5.0", "10.5.0"]
```

### For Local Development

```bash
# Pre-download all sources to avoid network retries
spack mirror create -d ~/spack-mirror slurm openmpi mysql

# Use local mirror for offline builds
slurm-factory build --mirror ~/spack-mirror

# Increase Docker resources for faster compilation
export DOCKER_CPUS=16
export DOCKER_MEMORY=32g
```

### For Production Deployments

```bash
# Use shared filesystem to deploy once, mount everywhere
tar -xzf slurm-25.11-gcc-13.4.0.tar.gz -C /shared/nfs/slurm/

# Nodes mount /shared/nfs/slurm -> /opt/slurm (read-only)
# No extraction needed on compute nodes

# Or use container registries for immutable deployments
skopeo copy dir:slurm-25.11-gcc-13.4.0 docker://registry/slurm:25.11
```

## Conclusion

Slurm Factory's three-tier buildcache strategy achieves:

- âš¡ **45x faster builds** for cached combinations (2 min vs 90 min)
- ğŸ”„ **100% consistency** across 36 Slurm Ã— GCC combinations
- ğŸ’¾ **40% storage savings** through intelligent deduplication
- ğŸŒ **Global CDN distribution** via CloudFront (no credentials needed)
- ğŸ”’ **Reproducible builds** guaranteed by Docker + public buildcache
- ğŸ“¦ **Modular architecture** enabling mix-and-match versions

**The result:** Any user can deploy any supported Slurm version with any supported GCC compiler in under 3 minutes, with zero configuration and guaranteed reproducibility.
